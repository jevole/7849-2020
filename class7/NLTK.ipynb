{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "### Installation\n",
    "\n",
    "On the command line ('cmd'), type: `pip install nltk`\n",
    "    \n",
    "Then, type: `python`. Within python (running on the command line), type: `import nltk` and `nltk.download()`\n",
    "        \n",
    "This will open up a window where you can select the different components to install. By default, everything is selected (which is good). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizing\n",
    "\n",
    "Tokenizers is used to divide strings into lists of substrings. For example, Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenizing',\n",
       " 'this',\n",
       " 'sentence',\n",
       " 'will',\n",
       " 'result',\n",
       " 'in',\n",
       " 'a',\n",
       " 'string',\n",
       " 'with',\n",
       " 'the',\n",
       " 'different',\n",
       " 'elements',\n",
       " '.',\n",
       " 'Very',\n",
       " 'exciting',\n",
       " 'indeed',\n",
       " '!']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('Tokenizing this sentence will result in a list with the different elements. Very exciting indeed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Text may contain stop words like 'the', 'is', 'are'. Stop words can be filtered from the text to be processed. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation\n",
    "\n",
    "As seen in the examples above, punctuation is part of the tokenized output and not filtered out by stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print (string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(data)\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "    if w not in stopWords and w not in string.punctuation:\n",
    "        wordsFiltered.append(w)\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy']\n"
     ]
    }
   ],
   "source": [
    "# as one-liner\n",
    "print ( [w for w in words if w not in stopWords and w not in string.punctuation] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple statistics for Apple 2017 MD&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 1663 samples and 6198 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Company', 209),\n",
       " ('sales', 115),\n",
       " ('2017', 101),\n",
       " ('’', 93),\n",
       " ('net', 92),\n",
       " ('2016', 88),\n",
       " ('billion', 70),\n",
       " ('tax', 55),\n",
       " ('2015', 49),\n",
       " ('“', 47),\n",
       " ('”', 47),\n",
       " ('ASU', 44),\n",
       " ('primarily', 36),\n",
       " ('due', 36),\n",
       " ('compared', 31)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# read Apple 2017 md&a\n",
    "with open('AAPL_2017.html' , 'r') as myfile:    \n",
    "    mda =  myfile.read() \n",
    "    \n",
    "# list of stop words and punctuation\n",
    "stopWords = set(stopwords.words('english') ) \n",
    "\n",
    "# tokens excluding stopwords\n",
    "mda_tokens = [x for x in word_tokenize(mda) if x.lower() not in stopWords and x not in string.punctuation]\n",
    "# convert it to nltk text\n",
    "text = nltk.Text(mda_tokens)\n",
    "# now we can use nltk functions on the text\n",
    "fdist2 = FreqDist(text)\n",
    "print(fdist2)\n",
    "fdist2.most_common(15)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple-compatible',\n",
       " 'Manufacturing-Related',\n",
       " 'available-for-sale',\n",
       " 'dollar-denominated',\n",
       " 'euro-denominated',\n",
       " 'exchange-related',\n",
       " 'headcount-related',\n",
       " 'industry-specific',\n",
       " 'infrastructure-related',\n",
       " 'manufacturing-related',\n",
       " 'other-than-temporarily',\n",
       " 'other-than-temporary',\n",
       " 'telecommunications',\n",
       " 'weighted-average']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# long words\n",
    "V = set(text)\n",
    "long_words = [w for w in V if len(w) > 15]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations: bigrams that occur more often than we would expect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigrams -- two words used together (both orders)\n",
    "from nltk import bigrams\n",
    "list(bigrams(['more', 'is', 'said', 'than', 'done']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2016', '2015'),\n",
       " ('2017', '2016'),\n",
       " ('2017', 'compared'),\n",
       " ('Company', '’'),\n",
       " ('Form', '10-K'),\n",
       " ('U.S.', 'dollar'),\n",
       " ('compared', '2016'),\n",
       " ('due', 'primarily'),\n",
       " ('net', 'sales'),\n",
       " ('relative', 'U.S.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(nltk.corpus.genesis.words('english-web.txt'))\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(mda_tokens2)\n",
    "scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "\n",
    "#sorted(bigram for bigram, score in scored) \n",
    "sorted(finder.nbest(bigram_measures.raw_freq, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'game', 'game', 'game']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print ( [ps.stem(w) for w in [\"game\",\"gaming\",\"gamed\",\"games\"]  ] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
